{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_data(filepath):\n",
    "    df = pd.read_csv(filepath, usecols=[0], header=None, encoding='unicode_escape')\n",
    "    df = df.iloc[1:,:]\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Name the first column\n",
    "    df.columns = [\"Raw\"]\n",
    "    \n",
    "    # Split the 'Raw' column by multiple spaces\n",
    "    split_data = df[\"Raw\"].str.split(r'\\s+', expand=True)\n",
    "    \n",
    "    # Extract Date and Time\n",
    "    df['Date'] = split_data[0] + ' ' + split_data[2]\n",
    "    \n",
    "    # Extract Data value\n",
    "    df['Data'] = split_data[3]\n",
    "    \n",
    "    # Drop the original 'Raw' column\n",
    "    df = df[['Date', 'Data']]\n",
    "    \n",
    "    # Convert 'Data' to numeric\n",
    "    df['Data'] = pd.to_numeric(df['Data'], errors='coerce')\n",
    "\n",
    "    df['Date'] = df['Date'].apply(pd.to_datetime, format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "    \n",
    "    # Drop rows with NaT in 'Date' column\n",
    "    df.dropna(subset='Date', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "flow_df = process_data(\"../../data-lake/HT/Hydrology/Arthurs Lake Spillway (418.1)/Inflow.csv\")\n",
    "#print(flow_df)\n",
    "\n",
    "temp_df = process_data(\"../../data-lake/HT/Hydrology/Arthurs Lake Spillway (418.1)/WQ at Morass Bay (418.24)/Continuous/Water Temp.csv\")\n",
    "#print(temp_df)\n",
    "\n",
    "sal_df = process_data(\"../../data-lake/HT/Hydrology/Arthurs Lake Spillway (418.1)/WQ at Morass Bay (418.24)/Continuous/salinity.csv\")\n",
    "#print(sal_df)\n",
    "\n",
    "chloa_df = process_data(\"../../data-lake/HT/Hydrology/Arthurs Lake Spillway (418.1)/WQ at Morass Bay (418.24)/Continuous/chloro_a.csv\")\n",
    "#print(chloa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Date   Data\n",
      "0   2015-05-15 12:20:00  0.010\n",
      "1   2015-07-15 11:35:00  0.010\n",
      "2   2015-09-10 10:32:00  0.010\n",
      "3   2015-11-11 10:30:00  0.010\n",
      "4   2016-01-28 10:35:00  0.010\n",
      "5   2016-03-31 15:40:00  0.010\n",
      "6   2016-05-26 12:30:00  0.010\n",
      "7   2016-07-21 12:20:00  0.010\n",
      "8   2016-09-14 12:40:00  0.010\n",
      "9   2016-11-24 13:00:00  0.010\n",
      "10  2017-01-27 15:15:00  0.010\n",
      "11  2017-02-22 12:40:00  0.010\n",
      "12  2017-04-28 09:20:00  0.010\n",
      "13  2017-06-21 16:30:00  0.010\n",
      "14  2017-07-25 17:00:00  0.010\n",
      "15  2017-09-11 16:40:00  0.010\n",
      "16  2017-10-18 10:15:00  0.010\n",
      "17  2017-11-23 17:05:00  0.010\n",
      "18  2018-01-11 18:10:00  0.010\n",
      "19  2018-03-22 15:10:00  0.010\n",
      "20  2018-05-08 14:20:00  0.010\n",
      "21  2018-07-26 14:30:00  0.010\n",
      "22  2018-09-26 12:30:00  0.010\n",
      "23  2018-11-28 10:45:00  0.010\n",
      "24  2019-01-09 16:35:00  0.010\n",
      "25  2019-02-13 17:25:00  0.010\n",
      "26  2019-03-12 16:20:00  0.010\n",
      "27  2019-04-16 14:04:00  0.010\n",
      "28  2019-05-22 09:30:00  0.010\n",
      "29  2019-06-20 12:00:00  0.010\n",
      "30  2019-07-23 17:00:00  0.010\n",
      "31  2019-08-27 14:50:00  0.010\n",
      "32  2019-09-26 09:10:00  0.010\n",
      "33  2019-10-23 09:00:00  0.011\n",
      "34  2019-11-19 15:10:00  0.010\n",
      "35  2019-12-16 12:55:00  0.010\n",
      "36  2020-01-23 12:07:00  0.010\n",
      "37  2020-02-17 15:45:00  0.010\n",
      "38  2020-05-13 13:40:00  0.010\n",
      "39  2020-06-10 14:45:00  0.010\n",
      "40  2020-08-13 10:00:00  0.011\n",
      "41  2020-10-20 14:30:00  0.010\n",
      "42  2020-12-22 14:15:00  0.010\n",
      "43  2021-02-24 11:10:00  0.010\n",
      "44  2021-04-29 13:55:00  0.011\n",
      "45  2021-06-22 12:40:00  0.011\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_processed_data(filepath):\n",
    "    df = pd.read_csv(filepath, encoding='unicode_escape')\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    df = df.loc[:,[\"Date\",\"Data\"]]\n",
    "\n",
    "    df.dropna(subset='Date', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "ss_df = process_processed_data(\"../../data-warehouse/csv/ht/alwq/ArthursLakeSpillway_TotalSuspendedSolids_profile_Data.csv\")\n",
    "\n",
    "nh4_df = process_processed_data(\"../../data-warehouse/csv/ht/alwq/ArthursLakeSpillway_Ammonium_profile_Data.csv\")\n",
    "\n",
    "no3_df = process_processed_data(\"../../data-warehouse/csv/ht/alwq/ArthursLakeSpillway_Nitrate_profile_Data.csv\")\n",
    "\n",
    "oxy_df = process_processed_data(\"../../data-warehouse/csv/ht/alwq/ArthursLakeSpillway_DissolvedOxygen_profile_Data.csv\")\n",
    "\n",
    "frp_df = process_processed_data(\"../../data-warehouse/csv/ht/alwq/ArthursLakeSpillway_FilteredReactivePhosphorus_profile_Data.csv\")\n",
    "\n",
    "tn_df = process_processed_data(\"../../data-warehouse/csv/ht/alwq/ArthursLakeSpillway_TotalNitrogen_profile_Data.csv\")\n",
    "\n",
    "tp_df = process_processed_data(\"../../data-warehouse/csv/ht/alwq/ArthursLakeSpillway_TotalPhosphorus_profile_Data.csv\")\n",
    "\n",
    "print(tp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Date      Data\n",
      "0      2016-01-07 12:00:00  0.865000\n",
      "1      2016-01-07 12:15:00  0.841875\n",
      "2      2016-01-07 12:30:00  0.818750\n",
      "3      2016-01-07 12:45:00  0.795625\n",
      "4      2016-01-07 13:00:00  0.772500\n",
      "...                    ...       ...\n",
      "292748 2024-05-13 23:00:00  0.998928\n",
      "292749 2024-05-13 23:15:00  0.984716\n",
      "292750 2024-05-13 23:30:00  0.970504\n",
      "292751 2024-05-13 23:45:00  0.956292\n",
      "292752 2024-05-14 00:00:00  0.942080\n",
      "\n",
      "[292753 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_dataframe(df, clip_data=True):\n",
    "\n",
    "    # Ensure 'Date' column is in datetime format\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Group by 'Date' and average the 'Data' values for duplicate timestamps\n",
    "    df = df.groupby('Date').mean().reset_index()\n",
    "    \n",
    "    # Set 'Date' as index for resampling\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Resample the DataFrame to 15-minute intervals and interpolate\n",
    "    df = df.resample('15T').interpolate(method='linear')\n",
    "\n",
    "    # Forward fill and backward fill to handle NaNs at the boundaries\n",
    "    df = df.ffill().bfill()\n",
    "    \n",
    "    # Reset the index to convert it back to a column\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Apply clipping only if specified and if the DataFrame is flow_df\n",
    "    if clip_data:\n",
    "        df['Data'] = df['Data'].clip(lower=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Assuming flow_df and temp_df are already defined DataFrames\n",
    "flow_df_processed = process_dataframe(flow_df.copy())\n",
    "temp_df_processed = process_dataframe(temp_df.copy(), clip_data=False)\n",
    "sal_df_processed = process_dataframe(sal_df.copy())\n",
    "chloa_df_processed = process_dataframe(chloa_df.copy())\n",
    "\n",
    "ss_df_processed = process_dataframe(ss_df.copy())\n",
    "nh4_df_processed = process_dataframe(nh4_df.copy())\n",
    "no3_df_processed = process_dataframe(no3_df.copy())\n",
    "oxy_df_processed = process_dataframe(oxy_df.copy())\n",
    "frp_df_processed = process_dataframe(frp_df.copy())\n",
    "tn_df_processed = process_dataframe(tn_df.copy())\n",
    "tp_df_processed = process_dataframe(tp_df.copy())\n",
    "\n",
    "print(chloa_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Date      Data\n",
      "0      2016-01-07 12:00:00  0.000865\n",
      "1      2016-01-07 12:15:00  0.000842\n",
      "2      2016-01-07 12:30:00  0.000819\n",
      "3      2016-01-07 12:45:00  0.000796\n",
      "4      2016-01-07 13:00:00  0.000772\n",
      "...                    ...       ...\n",
      "292748 2024-05-13 23:00:00  0.000999\n",
      "292749 2024-05-13 23:15:00  0.000985\n",
      "292750 2024-05-13 23:30:00  0.000971\n",
      "292751 2024-05-13 23:45:00  0.000956\n",
      "292752 2024-05-14 00:00:00  0.000942\n",
      "\n",
      "[292753 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "on_df_processed = tn_df_processed\n",
    "on_df_processed[\"Data\"] = tn_df_processed[\"Data\"] - no3_df_processed[\"Data\"] - nh4_df_processed[\"Data\"]\n",
    "\n",
    "op_df_processed = tp_df_processed\n",
    "op_df_processed[\"Data\"] = tp_df_processed[\"Data\"] - frp_df_processed[\"Data\"]\n",
    "\n",
    "chloa_df_processed[\"Data\"] *= 0.001\n",
    "print(chloa_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date columns to datetime format\n",
    "flow_df_processed['Date'] = pd.to_datetime(flow_df_processed['Date'])\n",
    "temp_df_processed['Date'] = pd.to_datetime(temp_df_processed['Date'])\n",
    "sal_df_processed['Date'] = pd.to_datetime(sal_df_processed['Date'])\n",
    "chloa_df_processed['Date'] = pd.to_datetime(chloa_df_processed['Date'])\n",
    "\n",
    "# Merge based on Date\n",
    "merged_df = pd.merge(flow_df_processed, temp_df_processed, on='Date', suffixes=('_flow', '_temp'), how='inner')\n",
    "\n",
    "# Merge the result with sal_df_processed based on Date, and rename the 'Data' column to 'Data_sal'\n",
    "merged_df = pd.merge(merged_df, sal_df_processed.rename(columns={'Data': 'Data_sal'}), on='Date', how='inner')\n",
    "\n",
    "merged_df = pd.merge(merged_df, chloa_df_processed.rename(columns={'Data': 'CHLA'}), on='Date', how='inner')\n",
    "\n",
    "# Display the merged DataFrame with required columns\n",
    "# print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006358720023168696\n",
      "                      Date  Data_flow  Data_temp  Data_sal      CHLA  \\\n",
      "0      2017-09-08 13:00:00   1.957236   3.610125  0.010000  0.000726   \n",
      "1      2017-09-08 13:15:00   4.253566   3.611094  0.010000  0.000722   \n",
      "2      2017-09-08 13:30:00   6.549896   3.612062  0.010000  0.000719   \n",
      "3      2017-09-08 13:45:00   8.846226   3.613031  0.010000  0.000715   \n",
      "4      2017-09-08 14:00:00  11.142556   3.614000  0.010000  0.000711   \n",
      "...                    ...        ...        ...       ...       ...   \n",
      "234184 2024-05-13 23:00:00   0.000000   8.563319  0.012611  0.000999   \n",
      "234185 2024-05-13 23:15:00   0.000000   8.556447  0.012608  0.000985   \n",
      "234186 2024-05-13 23:30:00   0.000000   8.549575  0.012605  0.000971   \n",
      "234187 2024-05-13 23:45:00   0.000000   8.542702  0.012602  0.000956   \n",
      "234188 2024-05-14 00:00:00   0.000000   8.535830  0.012599  0.000942   \n",
      "\n",
      "               SS       NH4       NO3        OXY       FRP        ON        OP  \n",
      "0        6.353990  0.005000  0.002000  11.142027  0.003000  0.277124  0.007000  \n",
      "1        6.353375  0.005000  0.002000  11.141804  0.003000  0.277120  0.007000  \n",
      "2        6.352760  0.005000  0.002000  11.141581  0.003000  0.277117  0.007000  \n",
      "3        6.352146  0.005000  0.002000  11.141359  0.003000  0.277113  0.007000  \n",
      "4        6.351531  0.005000  0.002000  11.141136  0.003000  0.277109  0.007000  \n",
      "...           ...       ...       ...        ...       ...       ...       ...  \n",
      "234184  10.342105  0.005196  0.002196  10.558710  0.003652  0.293658  0.006359  \n",
      "234185  10.342105  0.005196  0.002196  10.558710  0.003652  0.293658  0.006359  \n",
      "234186  10.342105  0.005196  0.002196  10.558710  0.003652  0.293658  0.006359  \n",
      "234187  10.342105  0.005196  0.002196  10.558710  0.003652  0.293658  0.006359  \n",
      "234188  10.342105  0.005196  0.002196  10.558710  0.003652  0.293658  0.006359  \n",
      "\n",
      "[234189 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "ss_df_processed['Date'] = pd.to_datetime(ss_df_processed['Date'])\n",
    "nh4_df_processed['Date'] = pd.to_datetime(nh4_df_processed['Date'])\n",
    "no3_df_processed['Date'] = pd.to_datetime(no3_df_processed['Date'])\n",
    "oxy_df_processed['Date'] = pd.to_datetime(oxy_df_processed['Date'])\n",
    "frp_df_processed['Date'] = pd.to_datetime(frp_df_processed['Date'])\n",
    "on_df_processed['Date'] = pd.to_datetime(on_df_processed['Date'])\n",
    "op_df_processed['Date'] = pd.to_datetime(op_df_processed['Date'])\n",
    "\n",
    "ss_mean = ss_df['Data'].mean()\n",
    "nh4_mean = nh4_df['Data'].mean()\n",
    "no3_mean = no3_df['Data'].mean()\n",
    "oxy_mean = oxy_df['Data'].mean()\n",
    "frp_mean = frp_df['Data'].mean()\n",
    "\n",
    "on_mean = on_df_processed['Data'].mean()\n",
    "op_mean = op_df_processed['Data'].mean()\n",
    "print(op_mean)\n",
    "\n",
    "merged_df = pd.merge(merged_df, ss_df_processed.rename(columns={'Data': 'SS'}), on='Date', how='left')\n",
    "merged_df = pd.merge(merged_df, nh4_df_processed.rename(columns={'Data': 'NH4'}), on='Date', how='left')\n",
    "merged_df = pd.merge(merged_df, no3_df_processed.rename(columns={'Data': 'NO3'}), on='Date', how='left')\n",
    "merged_df = pd.merge(merged_df, oxy_df_processed.rename(columns={'Data': 'OXY'}), on='Date', how='left')\n",
    "merged_df = pd.merge(merged_df, frp_df_processed.rename(columns={'Data': 'FRP'}), on='Date', how='left')\n",
    "merged_df = pd.merge(merged_df, on_df_processed.rename(columns={'Data': 'ON'}), on='Date', how='left')\n",
    "merged_df = pd.merge(merged_df, op_df_processed.rename(columns={'Data': 'OP'}), on='Date', how='left')\n",
    "\n",
    "merged_df =merged_df.fillna({'SS': ss_mean, 'NH4': nh4_mean, 'NO3': no3_mean, 'OXY': oxy_mean, 'FRP': frp_mean, 'ON': on_mean, 'OP': op_mean})\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed mean of DOC, NPOC - dis, NPOC - tot (Woods POC DOC.png), assumed\n",
    "merged_df['OC'] = 6\n",
    "\n",
    "merged_df['ZEROS'] = 0\n",
    "merged_df['ONES'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "merged_df.to_csv(\"inflow_woods_dam_20170908-20240514.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
