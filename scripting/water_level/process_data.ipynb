{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArthursLakeSpillway_WaterSurfaceHeight_profile_Data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# List of filepaths\n",
    "filepaths = [\n",
    "    \"../../data-lake/HT/Hydrology/Arthurs Lake Spillway (418.1)/WaterLevel.csv\"\n",
    "]\n",
    "\n",
    "# Load the mapping keys\n",
    "mapping_keys_df = pd.read_csv(\"mapping_keys.csv\")\n",
    "\n",
    "def process_data(filepath):\n",
    "    df = pd.read_csv(filepath, usecols=[0], header=None, encoding='unicode_escape')\n",
    "    df = df.iloc[1:,:]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.columns = [\"Raw\"]\n",
    "    \n",
    "    # Split the 'Raw' column by multiple spaces\n",
    "    split_data = df[\"Raw\"].str.split(r'\\s+', expand=True)\n",
    "    \n",
    "    # Extract Date and Time\n",
    "    df['Date'] = split_data[0] + ' ' + split_data[2]\n",
    "    \n",
    "    # Extract Data value\n",
    "    df['Data'] = split_data[3]\n",
    "    \n",
    "    # Drop the original 'Raw' column\n",
    "    df = df[['Date', 'Data']]\n",
    "\n",
    "    # Set values for Depth and QC\n",
    "    df['Depth'] = 0\n",
    "    df[\"QC\"] = \"N\"\n",
    "    \n",
    "    # Convert 'Data' to numeric\n",
    "    df['Data'] = pd.to_numeric(df['Data'], errors='coerce')\n",
    "\n",
    "    df['Date'] = df['Date'].apply(pd.to_datetime, format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "\n",
    "    # Remove rows with NaT in 'Date'\n",
    "    df = df.dropna(subset=['Date'])\n",
    "\n",
    "    df = df.sort_values(by='Date')\n",
    "    \n",
    "    # Extract the variable name from the URL\n",
    "    variable_match = re.search (r'/([^/]+)\\.csv$', filepath)\n",
    "    variable_name = variable_match.group(1) if variable_match else 'Unknown'\n",
    "    df['Variable'] = variable_name\n",
    "    #print(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_and_save_data(df, variable_name, output_filename):\n",
    "    # Filter rows where the variable is equal to the specified variable_name\n",
    "    variable_data = df.loc[df['Variable'] == variable_name]\n",
    "    #print(variable_data)\n",
    "\n",
    "    # Extract columns needed for the filtered data\n",
    "    filtered_data = variable_data.loc[:, [\"Variable\", \"Date\", \"Depth\", \"Data\", \"QC\"]]\n",
    "\n",
    "    # Replace empty cells with NaN\n",
    "    filtered_data.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "    # Convert value of different units\n",
    "    if variable_name in mapping_keys_df['Params.Name'].values:\n",
    "        conv_factor = mapping_keys_df.loc[mapping_keys_df['Params.Name'] == variable_name, 'Conv'].iloc[0]\n",
    "        filtered_data['Data'] = pd.to_numeric(filtered_data['Data'], errors='coerce')  # Convert non-numeric values to NaN\n",
    "        filtered_data['Data'] *= conv_factor\n",
    "\n",
    "    # Specify the directory path\n",
    "    directory = '../../data-warehouse/csv/ht/alwq'\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Write the filtered DataFrame to a CSV file in the specified directory\n",
    "    filtered_data.to_csv(os.path.join(directory, output_filename), index=False)\n",
    "\n",
    "# Specify the variables needed\n",
    "first_column = mapping_keys_df.iloc[:, 0]\n",
    "first_column.values.tolist()\n",
    "\n",
    "# Iterate over each URL\n",
    "for filepath in filepaths:\n",
    "    # Process the data for the current URL\n",
    "    df = process_data(filepath)\n",
    "    \n",
    "    # Replace empty cells with NaN\n",
    "    df.replace(\"\", np.nan, inplace=True)\n",
    "    \n",
    "    variable = df['Variable'].values[0]\n",
    "    # Filter mapping_keys_df to find the row corresponding to the variable\n",
    "    key_row = mapping_keys_df[mapping_keys_df['Params.Name'] == variable]\n",
    "    \n",
    "    # Extract the key value from the row\n",
    "    key_value = key_row['Key Value'].values[0] if not key_row.empty else None\n",
    "    \n",
    "    # Construct the output filename .replace(\" \", \"\")\n",
    "    output_filename = f'ArthursLakeSpillway_{key_value.replace(\" \", \"\")}_profile_Data.csv'\n",
    "    print(output_filename)\n",
    "    \n",
    "    # Filter and save data\n",
    "    filter_and_save_data(df, variable, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WoodsLakeMiddleContinuous_WaterSurfaceHeight_profile_Data.csv\n",
      "                       Date        Data  Depth QC    Variable\n",
      "1319982 2024-04-16 09:00:00  736.111023      0  N  WaterLevel\n",
      "1319983 2024-04-16 09:15:00  736.112976      0  N  WaterLevel\n",
      "1319984 2024-04-16 09:30:00  736.111023      0  N  WaterLevel\n",
      "1319985 2024-04-16 09:45:00  736.111023      0  N  WaterLevel\n",
      "1319986 2024-04-16 10:00:00  736.111023      0  N  WaterLevel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# List of filepaths\n",
    "filepaths = [\n",
    "    \"../../data-lake/HT/WaterLevel/462.1_WoodsLakeAtDam_WaterLevel_Continuous.csv\"\n",
    "]\n",
    "\n",
    "# Load the mapping keys\n",
    "mapping_keys_df = pd.read_csv(\"mapping_keys.csv\")\n",
    "\n",
    "def process_data(filepath):\n",
    "    df = pd.read_csv(filepath, usecols=[0], header=None, encoding='unicode_escape')\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.columns = [\"Raw\"]\n",
    "    \n",
    "    # Split the 'Raw' column by multiple spaces\n",
    "    split_data = df[\"Raw\"].str.split(r'\\s+', expand=True)\n",
    "    \n",
    "    # Extract Date and Time\n",
    "    df['Date'] = split_data[0] + ' ' + split_data[2]\n",
    "    \n",
    "    # Extract Data value\n",
    "    df['Data'] = split_data[3]\n",
    "    \n",
    "    # Drop the original 'Raw' column\n",
    "    df = df[['Date', 'Data']]\n",
    "\n",
    "    # Set values for Depth and QC\n",
    "    df['Depth'] = 0\n",
    "    df[\"QC\"] = \"N\"\n",
    "    \n",
    "    # Convert 'Data' to numeric\n",
    "    df['Data'] = pd.to_numeric(df['Data'], errors='coerce')\n",
    "\n",
    "    df['Date'] = df['Date'].apply(pd.to_datetime, format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "\n",
    "    # Remove rows with NaT in 'Date'\n",
    "    df = df.dropna(subset=['Date'])\n",
    "\n",
    "    df = df.sort_values(by='Date')\n",
    "    \n",
    "    # Extract the variable name from the URL\n",
    "    variable_match = re.search (r'[^/]+_(WaterLevel[^_]*)', filepath)\n",
    "    variable_name = variable_match.group(1) if variable_match else 'Unknown'\n",
    "    df['Variable'] = variable_name\n",
    "    #print(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_and_save_data(df, variable_name, output_filename):\n",
    "    # Filter rows where the variable is equal to the specified variable_name\n",
    "    variable_data = df.loc[df['Variable'] == variable_name]\n",
    "    print(variable_data.tail())\n",
    "\n",
    "    # Extract columns needed for the filtered data\n",
    "    filtered_data = variable_data.loc[:, [\"Variable\", \"Date\", \"Depth\", \"Data\", \"QC\"]]\n",
    "\n",
    "    # Replace empty cells with NaN\n",
    "    filtered_data.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "    # Convert value of different units\n",
    "    if variable_name in mapping_keys_df['Params.Name'].values:\n",
    "        conv_factor = mapping_keys_df.loc[mapping_keys_df['Params.Name'] == variable_name, 'Conv'].iloc[0]\n",
    "        filtered_data['Data'] = pd.to_numeric(filtered_data['Data'], errors='coerce')  # Convert non-numeric values to NaN\n",
    "        filtered_data['Data'] *= conv_factor\n",
    "\n",
    "    # Specify the directory path\n",
    "    directory = '../../data-warehouse/csv/ht/wlwq'\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Write the filtered DataFrame to a CSV file in the specified directory\n",
    "    filtered_data.to_csv(os.path.join(directory, output_filename), index=False)\n",
    "\n",
    "# Specify the variables needed\n",
    "first_column = mapping_keys_df.iloc[:, 0]\n",
    "first_column.values.tolist()\n",
    "\n",
    "# Iterate over each URL\n",
    "for filepath in filepaths:\n",
    "    # Process the data for the current URL\n",
    "    df = process_data(filepath)\n",
    "    \n",
    "    # Replace empty cells with NaN\n",
    "    df.replace(\"\", np.nan, inplace=True)\n",
    "    \n",
    "    variable = df['Variable'].values[0]\n",
    "    # Filter mapping_keys_df to find the row corresponding to the variable\n",
    "    key_row = mapping_keys_df[mapping_keys_df['Params.Name'] == variable]\n",
    "    \n",
    "    # Extract the key value from the row\n",
    "    key_value = key_row['Key Value'].values[0] if not key_row.empty else None\n",
    "    \n",
    "    # Construct the output filename .replace(\" \", \"\")\n",
    "    output_filename = f'WoodsLakeMiddleContinuous_{key_value.replace(\" \", \"\")}_profile_Data.csv'\n",
    "    print(output_filename)\n",
    "    \n",
    "    # Filter and save data\n",
    "    filter_and_save_data(df, variable, output_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
