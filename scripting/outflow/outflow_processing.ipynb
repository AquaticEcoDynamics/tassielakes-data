{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Date      Data\n",
      "0        11/03/1999 13:00:00  0.000000\n",
      "1        12/03/1999 02:45:00  0.000000\n",
      "2        12/03/1999 03:00:00  0.000000\n",
      "3        12/03/1999 11:00:00  0.000000\n",
      "4        12/03/1999 11:15:00  0.000000\n",
      "...                      ...       ...\n",
      "1263124  16/04/2024 09:00:00  0.380967\n",
      "1263125  16/04/2024 09:15:00  0.380967\n",
      "1263126  16/04/2024 09:30:00  0.380967\n",
      "1263127  16/04/2024 09:45:00  0.380967\n",
      "1263128  16/04/2024 10:00:00  0.380967\n",
      "\n",
      "[1263129 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#url = \"https://github.com/AquaticEcoDynamics/tassielakes-data/raw/main/data-lake/HT/Hydrology/454.1_LakeRiverBLWoodsLakeDam_ChannelFlow_Continuous.csv\"\n",
    "\n",
    "# Read the CSV file and select only the first column\n",
    "df = pd.read_csv(\"../../data-lake/HT/Hydrology/454.1_LakeRiverBLWoodsLakeDam_ChannelFlow_Continuous.csv\", usecols=[0], header=None, encoding='unicode_escape')\n",
    "#df = pd.read_csv(url, usecols=[0], header=None, encoding='unicode_escape')\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Name the first column\n",
    "df.columns = [\"Raw\"]\n",
    "\n",
    "# Split the 'Raw' column by multiple spaces\n",
    "split_data = df[\"Raw\"].str.split(r'\\s+', expand=True)\n",
    "#print(split_data)\n",
    "\n",
    "# Split the 'Raw' column by multiple spaces\n",
    "split_data = df[\"Raw\"].str.split(r'\\s+', expand=True)\n",
    "\n",
    "# Extract Date and Time\n",
    "df['Date'] = split_data[0] + ' ' + split_data[2]\n",
    "\n",
    "# Extract Data value\n",
    "df['Data'] = split_data[3]\n",
    "\n",
    "# Drop the original 'Raw' column\n",
    "df = df[['Date', 'Data']]\n",
    "\n",
    "# Convert 'Data' to numeric\n",
    "df['Data'] = pd.to_numeric(df['Data'], errors='coerce')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Date      Data\n",
      "0       1999-03-11 13:00:00  0.000000\n",
      "1       1999-03-12 02:45:00  0.000000\n",
      "2       1999-03-12 03:00:00  0.000000\n",
      "3       1999-03-12 11:00:00  0.000000\n",
      "4       1999-03-12 11:15:00  0.000000\n",
      "...                     ...       ...\n",
      "1263124 2024-04-16 09:00:00  0.380967\n",
      "1263125 2024-04-16 09:15:00  0.380967\n",
      "1263126 2024-04-16 09:30:00  0.380967\n",
      "1263127 2024-04-16 09:45:00  0.380967\n",
      "1263128 2024-04-16 10:00:00  0.380967\n",
      "\n",
      "[1263129 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Date' to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "\n",
    "# Drop rows with NaT in 'Date' column\n",
    "df = df.dropna(subset=['Date'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Date      Data\n",
      "0       11/03/1999 13:00:00  0.000000\n",
      "1       11/03/1999 13:15:00  0.000000\n",
      "2       11/03/1999 13:30:00  0.000000\n",
      "3       11/03/1999 13:45:00  0.000000\n",
      "4       11/03/1999 14:00:00  0.000000\n",
      "...                     ...       ...\n",
      "880112  16/04/2024 09:00:00  0.380967\n",
      "880113  16/04/2024 09:15:00  0.380967\n",
      "880114  16/04/2024 09:30:00  0.380967\n",
      "880115  16/04/2024 09:45:00  0.380967\n",
      "880116  16/04/2024 10:00:00  0.380967\n",
      "\n",
      "[880117 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Date' and average the 'Data' values for duplicate timestamps\n",
    "df = df.groupby('Date').mean().reset_index()\n",
    "\n",
    "# Set 'Date' as index for resampling\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Resample the DataFrame to 15-minute intervals and interpolate\n",
    "df = df.resample('15T').interpolate(method='linear')\n",
    "\n",
    "# Reset the index to convert it back to a column\n",
    "df = df.reset_index()\n",
    "\n",
    "# Resample the DataFrame to hourly intervals and interpolate\n",
    "# df = df.resample('H').interpolate(method='linear')\n",
    "\n",
    "# Format the 'Date' column\n",
    "df['Date'] = df['Date'].dt.strftime('%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv(\"outflow_woods_dam-spillway_19990311-20240416.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
