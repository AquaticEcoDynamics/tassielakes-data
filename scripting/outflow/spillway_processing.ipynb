{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Date  Data\n",
      "0        09/01/1968 09:00:00   0.0\n",
      "1        26/01/1968 09:00:00   0.0\n",
      "2        05/02/1968 09:00:00   0.0\n",
      "3        24/02/1968 09:00:00   0.0\n",
      "4        30/03/1968 09:00:00   0.0\n",
      "...                      ...   ...\n",
      "1319984  16/04/2024 09:30:00   0.0\n",
      "1319985  16/04/2024 09:45:00   0.0\n",
      "1319986  16/04/2024 10:00:00   0.0\n",
      "1319987  16/04/2024 10:15:00   0.0\n",
      "1319988  16/04/2024 10:30:00   0.0\n",
      "\n",
      "[1319989 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# url = \"https://github.com/AquaticEcoDynamics/tassielakes-data/raw/main/data-lake/HT/Hydrology/462.1_WoodsLakeAtDam_SpillwayDischarge_Continuous.csv\"\n",
    "\n",
    "# Read the CSV file and select only the first column\n",
    "df = pd.read_csv(\"../../data-lake/HT/Hydrology/462.1_WoodsLakeAtDam_SpillwayDischarge_Continuous.csv\", usecols=[0], header=None, encoding='unicode_escape')\n",
    "#df = pd.read_csv(url, usecols=[0], header=None, encoding='unicode_escape')\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Name the first column\n",
    "df.columns = [\"Raw\"]\n",
    "\n",
    "# Split the 'Raw' column by multiple spaces\n",
    "split_data = df[\"Raw\"].str.split(r'\\s+', expand=True)\n",
    "#print(split_data)\n",
    "\n",
    "# Split the 'Raw' column by multiple spaces\n",
    "split_data = df[\"Raw\"].str.split(r'\\s+', expand=True)\n",
    "\n",
    "# Extract Date and Time\n",
    "df['Date'] = split_data[0] + ' ' + split_data[2]\n",
    "\n",
    "# Extract Data value\n",
    "df['Data'] = split_data[3]\n",
    "\n",
    "# Drop the original 'Raw' column\n",
    "df = df[['Date', 'Data']]\n",
    "\n",
    "# Convert 'Data' to numeric\n",
    "df['Data'] = pd.to_numeric(df['Data'], errors='coerce')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Date  Data\n",
      "0       1968-01-09 09:00:00   0.0\n",
      "1       1968-01-26 09:00:00   0.0\n",
      "2       1968-02-05 09:00:00   0.0\n",
      "3       1968-02-24 09:00:00   0.0\n",
      "4       1968-03-30 09:00:00   0.0\n",
      "...                     ...   ...\n",
      "1319984 2024-04-16 09:30:00   0.0\n",
      "1319985 2024-04-16 09:45:00   0.0\n",
      "1319986 2024-04-16 10:00:00   0.0\n",
      "1319987 2024-04-16 10:15:00   0.0\n",
      "1319988 2024-04-16 10:30:00   0.0\n",
      "\n",
      "[1319978 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Date' to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "\n",
    "# Drop rows with NaT in 'Date' column\n",
    "df = df.dropna(subset=['Date'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Date  Data\n",
      "0       1968-01-09 09:00:00   0.0\n",
      "1       1968-01-09 09:15:00   0.0\n",
      "2       1968-01-09 09:30:00   0.0\n",
      "3       1968-01-09 09:45:00   0.0\n",
      "4       1968-01-09 10:00:00   0.0\n",
      "...                     ...   ...\n",
      "1972994 2024-04-16 09:30:00   0.0\n",
      "1972995 2024-04-16 09:45:00   0.0\n",
      "1972996 2024-04-16 10:00:00   0.0\n",
      "1972997 2024-04-16 10:15:00   0.0\n",
      "1972998 2024-04-16 10:30:00   0.0\n",
      "\n",
      "[1972999 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Date' and average the 'Data' values for duplicate timestamps\n",
    "df = df.groupby('Date').mean().reset_index()\n",
    "\n",
    "# Set 'Date' as index for resampling\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Resample the DataFrame to 15-minute intervals and interpolate\n",
    "df = df.resample('15T').interpolate(method='linear')\n",
    "\n",
    "# Reset the index to convert it back to a column\n",
    "df = df.reset_index()\n",
    "\n",
    "# Resample the DataFrame to hourly intervals and interpolate\n",
    "# df = df.resample('H').interpolate(method='linear')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv(\"Interpolated_462.1_WoodsLakeAtDam_SpillwayDischarge_Continuous.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
