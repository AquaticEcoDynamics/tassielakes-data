{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WoodsLakeAtDam_Precipitation_profile_Data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# List of filepaths\n",
    "filepaths = [\n",
    "    \"../../data-lake/HT/Meteorology/Rainfall Woods Lake At Dam.csv\"\n",
    "]\n",
    "\n",
    "# Load the mapping keys\n",
    "mapping_keys_df = pd.read_csv(\"mapping_keys_wl.csv\")\n",
    "\n",
    "def process_data(filepath):\n",
    "    df = pd.read_csv(filepath, usecols=[0], header=None, encoding='unicode_escape')\n",
    "    df = df.iloc[1:,:]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.columns = [\"Raw\"]\n",
    "    \n",
    "    # Split the 'Raw' column by multiple spaces\n",
    "    split_data = df[\"Raw\"].str.split(r'\\s+', expand=True)\n",
    "    \n",
    "    # Extract Date and Time\n",
    "    df['Date'] = split_data[0] + ' ' + split_data[2]\n",
    "    \n",
    "    # Extract Data value\n",
    "    df['Data'] = split_data[3]\n",
    "    \n",
    "    # Drop the original 'Raw' column\n",
    "    df = df[['Date', 'Data']]\n",
    "\n",
    "    # Set values for Depth and QC\n",
    "    df['Depth'] = 0\n",
    "    df[\"QC\"] = \"N\"\n",
    "    \n",
    "    # Convert 'Data' to numeric\n",
    "    df['Data'] = pd.to_numeric(df['Data'], errors='coerce')\n",
    "\n",
    "    df['Date'] = df['Date'].apply(pd.to_datetime, format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "    df = df.sort_values(by='Date')\n",
    "    \n",
    "    # Extract the variable name from the URL\n",
    "    # variable_match = re.search (r'[^/]+_(ChannelFlow[^_]*)', filepath)\n",
    "    # variable_name = variable_match.group(1) if variable_match else 'Unknown'\n",
    "    df['Variable'] = \"Rainfall\"\n",
    "    #print(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_and_save_data(df, variable_name, output_filename):\n",
    "    # Filter rows where the variable is equal to the specified variable_name\n",
    "    variable_data = df.loc[df['Variable'] == variable_name]\n",
    "    #print(variable_data)\n",
    "\n",
    "    # Extract columns needed for the filtered data\n",
    "    filtered_data = variable_data.loc[:, [\"Variable\", \"Date\", \"Depth\", \"Data\", \"QC\"]]\n",
    "\n",
    "    # Replace empty cells with NaN\n",
    "    filtered_data.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "    # Convert value of different units\n",
    "    if variable_name in mapping_keys_df['Params.Name'].values:\n",
    "        conv_factor = mapping_keys_df.loc[mapping_keys_df['Params.Name'] == variable_name, 'Conv'].iloc[0]\n",
    "        filtered_data['Data'] = pd.to_numeric(filtered_data['Data'], errors='coerce')  # Convert non-numeric values to NaN\n",
    "        filtered_data['Data'] *= conv_factor\n",
    "\n",
    "    # Specify the directory path\n",
    "    directory = '../../data-warehouse/csv/ht/met'\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Write the filtered DataFrame to a CSV file in the specified directory\n",
    "    filtered_data.to_csv(os.path.join(directory, output_filename), index=False)\n",
    "\n",
    "# Specify the variables needed\n",
    "first_column = mapping_keys_df.iloc[:, 0]\n",
    "first_column.values.tolist()\n",
    "\n",
    "# Iterate over each URL\n",
    "for filepath in filepaths:\n",
    "    # Process the data for the current URL\n",
    "    df = process_data(filepath)\n",
    "    \n",
    "    # Replace empty cells with NaN\n",
    "    df.replace(\"\", np.nan, inplace=True)\n",
    "    \n",
    "    variable = df['Variable'].values[0]\n",
    "    # Filter mapping_keys_df to find the row corresponding to the variable\n",
    "    key_row = mapping_keys_df[mapping_keys_df['Params.Name'] == variable]\n",
    "    \n",
    "    # Extract the key value from the row\n",
    "    key_value = key_row['Key Value'].values[0] if not key_row.empty else None\n",
    "    \n",
    "    # Construct the output filename .replace(\" \", \"\")\n",
    "    output_filename = f'WoodsLakeAtDam_{key_value.replace(\" \", \"\")}_profile_Data.csv'\n",
    "    print(output_filename)\n",
    "    \n",
    "    # Filter and save data\n",
    "    filter_and_save_data(df, variable, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOM_AirTemperature_profile_Data.csv\n",
      "BOM_RelativeHumidity_profile_Data.csv\n",
      "BOM_Precipitation_profile_Data.csv\n",
      "BOM_WindDirection_profile_Data.csv\n",
      "BOM_WindSpeed_profile_Data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# List of filepaths\n",
    "filepaths = [\n",
    "    \"../../data-lake/BOM/HT/Hourly aggregate/AirTemp.csv\",\n",
    "    \"../../data-lake/BOM/HT/Hourly aggregate/Humidity.csv\",\n",
    "    \"../../data-lake/BOM/HT/Hourly aggregate/Rainfall.csv\",\n",
    "    \"../../data-lake/BOM/HT/Hourly aggregate/WD.csv\",\n",
    "    \"../../data-lake/BOM/HT/Hourly aggregate/WS.csv\"\n",
    "]\n",
    "\n",
    "# Load the mapping keys\n",
    "mapping_keys_df = pd.read_csv(\"mapping_keys_bom.csv\")\n",
    "\n",
    "def process_data(filepath):\n",
    "    df = pd.read_csv(filepath, header=None, encoding='unicode_escape')\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Extract Date and Time\n",
    "    df['Date'] = df[0] + ' ' + df[2]\n",
    "    \n",
    "    # Extract Data value\n",
    "    df['Data'] = df[3]\n",
    "    \n",
    "    # Drop the original 'Raw' column\n",
    "    df = df[['Date', 'Data']]\n",
    "\n",
    "    # Set values for Depth and QC\n",
    "    df['Depth'] = 0\n",
    "    df[\"QC\"] = \"N\"\n",
    "    \n",
    "    # Convert 'Data' to numeric\n",
    "    df['Data'] = pd.to_numeric(df['Data'], errors='coerce')\n",
    "\n",
    "    df['Date'] = df['Date'].apply(pd.to_datetime, format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "    df = df.sort_values(by='Date')\n",
    "    \n",
    "    # Extract the variable name from the URL\n",
    "    variable_match = re.search (r'/([^/]+)\\.csv$', filepath)\n",
    "    variable_name = variable_match.group(1) if variable_match else 'Unknown'\n",
    "    df['Variable'] = variable_name\n",
    "    #print(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_and_save_data(df, variable_name, output_filename):\n",
    "    # Filter rows where the variable is equal to the specified variable_name\n",
    "    variable_data = df.loc[df['Variable'] == variable_name]\n",
    "    #print(variable_data)\n",
    "\n",
    "    # Extract columns needed for the filtered data\n",
    "    filtered_data = variable_data.loc[:, [\"Variable\", \"Date\", \"Depth\", \"Data\", \"QC\"]]\n",
    "\n",
    "    # Replace empty cells with NaN\n",
    "    filtered_data.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "    # Convert value of different units\n",
    "    if variable_name in mapping_keys_df['Params.Name'].values:\n",
    "        conv_factor = mapping_keys_df.loc[mapping_keys_df['Params.Name'] == variable_name, 'Conv'].iloc[0]\n",
    "        filtered_data['Data'] = pd.to_numeric(filtered_data['Data'], errors='coerce')  # Convert non-numeric values to NaN\n",
    "        filtered_data['Data'] *= conv_factor\n",
    "\n",
    "    # Specify the directory path\n",
    "    directory = '../../data-warehouse/csv/ht/met'\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Write the filtered DataFrame to a CSV file in the specified directory\n",
    "    filtered_data.to_csv(os.path.join(directory, output_filename), index=False)\n",
    "\n",
    "# Specify the variables needed\n",
    "first_column = mapping_keys_df.iloc[:, 0]\n",
    "first_column.values.tolist()\n",
    "\n",
    "# Iterate over each URL\n",
    "for filepath in filepaths:\n",
    "    # Process the data for the current URL\n",
    "    df = process_data(filepath)\n",
    "    \n",
    "    # Replace empty cells with NaN\n",
    "    df.replace(\"\", np.nan, inplace=True)\n",
    "    \n",
    "    variable = df['Variable'].values[0]\n",
    "    # Filter mapping_keys_df to find the row corresponding to the variable\n",
    "    key_row = mapping_keys_df[mapping_keys_df['Params.Name'] == variable]\n",
    "    \n",
    "    # Extract the key value from the row\n",
    "    key_value = key_row['Key Value'].values[0] if not key_row.empty else None\n",
    "    \n",
    "    # Construct the output filename .replace(\" \", \"\")\n",
    "    output_filename = f'BOM_{key_value.replace(\" \", \"\")}_profile_Data.csv'\n",
    "    print(output_filename)\n",
    "    \n",
    "    # Filter and save data\n",
    "    filter_and_save_data(df, variable, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Date  Data   QC  Depth\n",
      "0  2000-01-01 00:00:00   NaN  NaN      0\n",
      "1  2000-01-02 00:00:00   NaN  NaN      0\n",
      "2  2000-01-03 00:00:00   NaN  NaN      0\n",
      "3  2000-01-04 00:00:00   NaN  NaN      0\n",
      "4  2000-01-05 00:00:00   NaN  NaN      0\n",
      "BOMIDC_Precipitation_profile_Data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# List of filepaths\n",
    "filepaths = [\n",
    "    \"../../data-lake/BOM/IDC/IDCJAC0009_096033_1800_Data.csv\"\n",
    "]\n",
    "\n",
    "# Load the mapping keys\n",
    "mapping_keys_df = pd.read_csv(\"mapping_keys_bomidc.csv\")\n",
    "\n",
    "def process_data(filepath):\n",
    "    df = pd.read_csv(filepath, encoding='unicode_escape')\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Extract Date and Time\n",
    "    # time_delta = pd.Timedelta(hours=0, minutes=0, seconds=0)\n",
    "    # time_str = f\"{time_delta.components.hours:02}:{time_delta.components.minutes:02}:{time_delta.components.seconds:02}\"\n",
    "    # df['Time'] = time_str\n",
    "    df['Date'] = df.iloc[:, 4].astype(str) + '/' + df.iloc[:, 3].astype(str) + '/' + df.iloc[:, 2].astype(str)\n",
    "    \n",
    "    # df['Date'] = df['YMD'] + \" \" + df['Time'] \n",
    "    \n",
    "    # Extract Data value\n",
    "    df['Data'] = df.iloc[:, 5]\n",
    "\n",
    "    # Extract QC\n",
    "    df['QC'] = df.iloc[:, 7]\n",
    "    \n",
    "    # Drop the original 'Raw' column\n",
    "    df = df[['Date', 'Data', 'QC']]\n",
    "\n",
    "    # Set values for Depth\n",
    "    df['Depth'] = 0\n",
    "    \n",
    "    # Convert 'Data' to numeric\n",
    "    df['Data'] = pd.to_numeric(df['Data'], errors='coerce')\n",
    "\n",
    "    # df['Date'] = df['Date'].apply(pd.to_datetime, format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "    # Assuming your DataFrame is named df\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y') + pd.to_timedelta('00:00:00')\n",
    "\n",
    "    # If you want to convert it back to a string with the specified format\n",
    "    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df = df.sort_values(by='Date')\n",
    "    print(df.head())\n",
    "    \n",
    "    # Extract the variable name from the URL\n",
    "    # variable_match = re.search (r'/([^/]+)\\.csv$', filepath)\n",
    "    # variable_name = variable_match.group(1) if variable_match else 'Unknown'\n",
    "    df['Variable'] = \"IDCJAC0009\"\n",
    "    #print(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_and_save_data(df, variable_name, output_filename):\n",
    "    # Filter rows where the variable is equal to the specified variable_name\n",
    "    variable_data = df.loc[df['Variable'] == variable_name]\n",
    "    #print(variable_data)\n",
    "\n",
    "    # Extract columns needed for the filtered data\n",
    "    filtered_data = variable_data.loc[:, [\"Variable\", \"Date\", \"Depth\", \"Data\", \"QC\"]]\n",
    "\n",
    "    # Replace empty cells with NaN\n",
    "    filtered_data.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "    # Convert value of different units\n",
    "    if variable_name in mapping_keys_df['Params.Name'].values:\n",
    "        conv_factor = mapping_keys_df.loc[mapping_keys_df['Params.Name'] == variable_name, 'Conv'].iloc[0]\n",
    "        filtered_data['Data'] = pd.to_numeric(filtered_data['Data'], errors='coerce')  # Convert non-numeric values to NaN\n",
    "        filtered_data['Data'] *= conv_factor\n",
    "\n",
    "    # Specify the directory path\n",
    "    directory = '../../data-warehouse/csv/ht/met'\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Write the filtered DataFrame to a CSV file in the specified directory\n",
    "    filtered_data.to_csv(os.path.join(directory, output_filename), index=False)\n",
    "\n",
    "# Specify the variables needed\n",
    "first_column = mapping_keys_df.iloc[:, 0]\n",
    "first_column.values.tolist()\n",
    "\n",
    "# Iterate over each URL\n",
    "for filepath in filepaths:\n",
    "    # Process the data for the current URL\n",
    "    df = process_data(filepath)\n",
    "    \n",
    "    # Replace empty cells with NaN\n",
    "    df.replace(\"\", np.nan, inplace=True)\n",
    "    \n",
    "    variable = df['Variable'].values[0]\n",
    "    # Filter mapping_keys_df to find the row corresponding to the variable\n",
    "    key_row = mapping_keys_df[mapping_keys_df['Params.Name'] == variable]\n",
    "    \n",
    "    # Extract the key value from the row\n",
    "    key_value = key_row['Key Value'].values[0] if not key_row.empty else None\n",
    "    \n",
    "    # Construct the output filename .replace(\" \", \"\")\n",
    "    output_filename = f'BOMIDC_{key_value.replace(\" \", \"\")}_profile_Data.csv'\n",
    "    print(output_filename)\n",
    "    \n",
    "    # Filter and save data\n",
    "    filter_and_save_data(df, variable, output_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
